---
title: "Final Project For Statistical Learning II"
author: "Ivan Padezhki"
date: "April 20, 2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can embed an R code chunk like this:

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.

# Libraries

```{r}
library(tidyverse)
library(corrplot)
library(Hmisc)
```

# 1. Input of data
## 1.1. Dataset Suicide rates overview 

<https://www.kaggle.com/datasets/russellyates88/suicide-rates-overview-1985-to-2016>


```{r}
data_full <- read.csv("data/master.csv")

names(data_full)
```
```{r}
summary(data_full)
```
How did we obtain the data
### 1.1.1. Data Cleaning and Filtering organize
and tidy the data

```{r}
# select only 2016
sum(data_full$year == 2016)
```
```{r}
hist(data_full)
```



```{r}
# check correlation between variables
df_cor <- data_full %>% mutate_if(is.character, as.factor)
df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
#run a correlation and drop the insignificant ones
corr <- cor(df_cor, use = 'pairwise.complete.obs')
#prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag=TRUE)] <- NA 
# print the corr matrix
# print(corr)
```

plot the corr matrix

```{r}
#plot correlations visually
corrplot(corr, is.corr=FALSE, tl.col="black", na.label=" ")
```

## 1.2. Reddit dataset with suicide questionnaire
<https://www.kaggle.com/datasets/kingburrito666/the-demographic-rforeveralone-dataset> 
```{r}
data_quest <- read.csv("data/foreveralone.csv")
names(data_quest)
```
```{r}
summary(data_quest)
```

```{r}
hist(data_quest)
```
## 1.3. HFI dataset
```{r}
data_hfi <- read.csv('data/hfi_cc_2019.csv')
#names(data_hfi)
data_pf <- data_hfi[1:69]
data_pf_global <- select(data_pf, "year", "countries", "region", "hf_score", "pf_rol", "pf_ss", "pf_movement", "pf_religion", "pf_association", "pf_expression", "pf_identity", "pf_score", "pf_rank")
describe(data_pf_global)
```

```{r}
data_pf_2019 <- data_pf_global[data_pf_global$year== 2019, ]

```
## 1.4. WHO suicide rates
```{r}
data_who <- read.csv('data/who_rates.csv')
names(data_who)
data_who_var_names <- c('Period', 'Location', 'FactValueNumeric')
data_who_var <- select(data_who, data_who_var_names)
```
```{r}
describe(data_who_var)
hist(data_who_var$FactValueNumeric)
```

```{r}
sum(data_who_var$Period==2019)

data_who_2019 <- data_who_var[data_who_var$Period==2019, ]
sum(data_who_2019$Period != 2019)
```

# 2. Overall cleaning of datasets and combination
## 2.2. Identifying Missings
End up with a data in a standardized format across all data

Can we merge the two data sets based on country/location? 
They differ quite a bit
```{r}
data_pf_global$countries
data_who_2019$Location
```


```{r}
data_who_2019$Location == data_pf_2019$countries
data_comb <- merge(data_who_2019, data_pf_2019, by.x = 'Location', by.y = 'region', all = TRUE)
describe(data_comb)
```
# 3. Data Exploration

figure out what relevant questions there are in the data Inspect data
and its features what data types are there (numerical, categorical,
ordinal) - they require different treatments

## 3.1. Descriptive statistics

visualizations: identify patterns and trends in the data

# 4. Model data

Use regression and predictions for forecasting future values Or use
classification to identify groups

Reduce dimensionality of the data set - not all our values are
essentials to predicting the model

Select relevant features that contribute to the prediction of results or
apply shrinkage regressions

# 5. Interpret data

How do we present our analysis: basically answer the questions we set in
the beginning using our results - technical details are necessary but
make them accessible - tell a story
