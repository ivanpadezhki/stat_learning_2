---
title: "FINAL_stat"
author: "ET"
date: "06/06/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(Hmisc)
library(corrplot)
library(RColorBrewer)
library(stats)
library(readr)
```

# 1. Input of data

## 1.1. Dataset Breast Cancer

<https://www.kaggle.com/code/leemun1/predicting-breast-cancer-logistic-regression/data>,
first accessed 30/05/2022

```{r}
library(readr)
ds <- read_csv("https://raw.githubusercontent.com/ivanpadezhki/stat_learning_2/master/data/diabetes_dataset__2019.csv?token=GHSAT0AAAAAABTXCTNAD4HUUN5MUAT2TSDEYU6D7XA")

```
```{r}
ds[1:5,]
```


Get values for categorical variables

```{r}
for(i in 1:ncol(ds)) {       # for-loop over columns
  print(lapply(ds[i], unique))
}
```

Change all categorical variables to single dummies (e.g. for "Age", base-level will be "less than 40",
then we will have 3 dummies for each of the levels specified)


# 2. Overall cleaning

##2.1. Normalizing value names 

```{r}

ds_fin <- ds

#Fixing the variable Pdiabetes 

unique(ds_fin['Pdiabetes'])

ds_fin$Pdiabetes[ds_fin$Pdiabetes == ''] <- NA
ds_fin$Pdiabetes[ds_fin$Pdiabetes == ' no'] <- 'no'
ds_fin$Diabetic[ds_fin$Diabetic == '0'] <- 'no'

#Fixing the variable Diabetic

unique(ds_fin['Diabetic'])

ds_fin$Diabetic[ds_fin$Diabetic == ' no'] <- 'no'
ds_fin$Diabetic[ds_fin$Diabetic == ''] <- NA


#Fixing the variable BPLevel

unique(ds_fin$BPLevel)

ds_fin$BPLevel[ds_fin$BPLevel == 'High'] <- 'high'
ds_fin$BPLevel[ds_fin$BPLevel == 'Low'] <- 'low'
ds_fin$BPLevel[ds_fin$BPLevel == 'normal '] <- 'normal'

```

## 2.2. Identifying Missing Values


```{r}
sum(is.na(ds_fin)==TRUE)
```

#Removing rows which include NAs 
```{r}
ds_fin <- na.omit(ds_fin)
```

##2.3  TUrning categorical variables into single-level dummies

```{r}
as.data.frame(colnames(ds_fin))
```
```{r}
unique(ds_fin$Age)
ds_fin$Age40_49 <- ifelse(ds_fin$Age == "40-49", 1, 0)
ds_fin$Age50_59 <- ifelse(ds_fin$Age == "50-59", 1, 0)
ds_fin$Age_60 <- ifelse(ds_fin$Age == "60 or older", 1, 0)
ds_fin[0:5,]
ds_fin$Age <- NULL

unique(ds_fin$Gender)
ds_fin$Male <- ifelse(ds_fin$Gender == "Male", 1, 0)
unique(ds_fin$Male)
ds_fin$Gender <- NULL

unique(ds_fin$Family_Diabetes)
ds_fin$Family_Diabetes <- ifelse(ds_fin$Family_Diabetes == "yes", 1, 0)
unique(ds_fin$Family_Diabetes)


unique(ds_fin$highBP)
ds_fin$highBP <- ifelse(ds_fin$highBP == "yes", 1, 0)
unique(ds_fin$highBP)


unique(ds_fin$PhysicallyActive)
ds_fin$PhysLow <- ifelse(ds_fin$PhysicallyActive == "less than half an hr", 1, 0)
ds_fin$PhysMid <- ifelse(ds_fin$PhysicallyActive == "more than half an hr", 1, 0)
ds_fin$PhysHigh <- ifelse(ds_fin$PhysicallyActive == "one hr or more", 1, 0)
ds_fin$PhysicallyActive <- NULL 


unique(ds$Smoking)
ds_fin$Smoking <- ifelse(ds_fin$Smoking == "yes", 1, 0)

unique(ds_fin$Alcohol)
ds_fin$Alcohol <- ifelse(ds_fin$Alcohol == "yes", 1, 0)

unique(ds_fin$RegularMedicine)
ds_fin$RegularMedicine <- ifelse(ds_fin$RegularMedicine == "yes", 1, 0)

unique(ds$JunkFood)
ds_fin$JunkOften <- ifelse(ds_fin$JunkFood == "often", 1, 0)
ds_fin$JunkVeryOften <- ifelse(ds_fin$JunkFood == "very often", 1, 0)
ds_fin$JunkAlways <- ifelse(ds_fin$JunkFood == "always", 1, 0)
ds_fin$JunkFood <- NULL


unique(ds_fin$Stress)
ds_fin$StressSometimes <- ifelse(ds_fin$Stress == "sometimes", 1, 0)
ds_fin$StressOften <- ifelse(ds_fin$Stress == "very often", 1, 0)
ds_fin$StressAlways <- ifelse(ds_fin$Stress == "always", 1, 0)
ds_fin$Stress <- NULL

unique(ds_fin$BPLevel)
ds_fin$BPLow <- ifelse(ds_fin$BPLevel == "low", 1, 0)
ds_fin$BPNormal <- ifelse(ds_fin$BPLevel == "normal", 1, 0)
ds_fin$BPLevel <- NULL

unique(ds$Pdiabetes)
ds_fin$Pdiabetes <- ifelse(ds_fin$Pdiabetes == "yes", 1, 0)

unique(ds_fin$UriationFreq)
ds_fin$UriationFreq <- ifelse(ds_fin$UriationFreq == "quite often", 1, 0)

unique(ds_fin$Diabetic)
ds_fin$Diabetic <- ifelse(ds_fin$Diabetic == "yes", 1, 0)

ds_fin[1:5,]

```
Checking every column is the correct data type 

```{r}
for(i in 1:ncol(ds_fin)) {       # for-loop over columns
  print(lapply(ds_fin[i], class))
}
```
#3.Modeling the data: LOGISTIC REGRESSION 
##3.1 Full model 
```{r}
log_reg <- glm(Diabetic ~., data = ds_fin, family = binomial)
summary(log_reg)
```

##3.2 Diagnostics 

First, we look at the correlation plot of our variables 
```{r}
library(ggcorrplot)
model.matrix(~0+., data=ds_fin) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
##3.2 Variable selection 

We try dropping the variable "Sleep" since it is heavily correlated (0.54) with the variable "Sound Sleep" AND its P-value in the logistic regression is close to 1. 

```{r}

log_reg_noSleep <- glm(Diabetic ~. -Sleep, data = ds_fin, family = binomial)
summary(log_reg_noSleep)

## AIC drops to 536.77, CONFIRM dropping variable Sleep  
```
Now we produce a new correlation matrix with the new dataset 

```{r}

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-6]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

Now we can observe that the variables Smoking and Alcohol are heavily correlated (0.52) AND Alcohol is not significant 
in the regression with p-value =0.27. Hence, we try dropping the variable Alcohol. 

```{r}
log_reg_noSleep_noAlcohol <- glm(Diabetic ~. -Sleep-Alcohol, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol)

## AIC drops to 536.01: CONFIRM dropping variable Alcohol 
```
```{r}
library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(5,6)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

Now, we can observe highBP remains non-significant in all models tried so far (p-value=0.24) AND it is heavily correlated to 
BPNormal (-0.64). We attempt two regressions, one dropping highBP and the other one dropping BPNormal in order to understand 
which variable to keep. 

```{r}
log_reg_noSleep_noAlcohol_noBPNormal <- glm(Diabetic ~. -Sleep-Alcohol-BPNormal, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_noBPNormal)
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP <- glm(Diabetic ~. -Sleep-Alcohol-highBP, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP)

## AIC drops to 535.29: CONFIRM dropping highBP, keep BPNormal bc AIC higher for previous regression
```
```{r}
library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now we observe that variable PhysMid is heavily correlated with variable PhysLow AND variable PhysMid is not significant (p-value=0.22). Hence, we try dropping this variable. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid)

## AIC drops to 534.78: CONFIRM dropping PhysMid
```
```{r}

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6,18)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

```
Now, since the not significant variables StressOften (p-value=0.47) and StressSometimes(p-value=0.38) are heavily correlated (-0.55), we run two models: one without one, and one without the other, to figure out which one should be dropped. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften)

##AIC drops to 533.31: CONFIRM dropping the variable StressOften since the other AIC is lower 
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressSometimes <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressSometimes, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressSometimes)

colnames(ds_fin)

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6,18,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we observe the non-significant variables StressAlways (p-value=0.211927) and StressSometimes (p-value=0.612708) are heavily correlated (-0.39).we run two models: one without one, and one without the other, to figure out which one should be dropped.

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Always <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Always)
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes)

##AIC drops to 531.57: CONFIRM dropping variable StressSometimes
```
```{r}
colnames(ds_fin)

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6,18,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Moreover, the variable Pregancies (i.e. n. of pregnancies) is misleading our model since it is heavily correlated to the Male variable(-0.5) for obvious reasons. Since papers on the subejct (FIND A SPECIFIC ONE) point to gestational diabetes as an important factor in order to develop diabetes type 2, rather than pregnancy per se, we try dropping this variable. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregancies, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg)

##AIC drops to 531.02: CONFIRM dropping variable Pregancies 
```

```{r}
colnames(ds_fin)

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6,9,18,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we can observe variables JunkAlways and StressAlways are heavily correlated (0.28) AND the variable JunkAlways is not significant (p-value=0.63), while STressAlways is somewhat significant. We try dropping both and observe the AIC. 
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregancies-JunkAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways)

## AIC drops to 529.25: CONFIRM dropping variable JunkAlways 


```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregancies-StressAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways) ##NO, AIC gets higher 

```
```{r}
colnames(ds_fin)

library(ggcorrplot)
model.matrix(~0+., data=ds_fin[,-c(2,5,6,9,18,22,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we see that variable BPLow is not significant AND is heavily correlated with BPNormal (-0.29). We try dropping both and observe the AIC.

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPlow <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregancies-StressAlways-BPLow, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPlow) ##NO, AIC gets higher 

```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPNormal <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregancies-StressAlways-BPNormal, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPNormal) ##NO, AIC gets higher
```
In both cases, AIC gets higher so we DO NOT CONFIRM dropping these variables. 

##NB: 
Since AIC works well asymptotically (i.e. when we have many more observations than variables) and our models our quite large, I now calculate the BIC (Bayesian Information Criterion) for all models so far to make sure we are dropping variables in a sensible way. 

```{r}
#install.packages('flexmix')
library(flexmix)

bic_1 <- BIC(log_reg)
bic_2 <- BIC(log_reg_noSleep)
bic_3 <- BIC(log_reg_noSleep_noAlcohol)
bic_4 <- BIC(log_reg_noSleep_noAlcohol_nohighBP)
bic_5 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid)
bic_6 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften)
bic_7 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes)
bic_8 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg)
bic_9 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways)

isTRUE((bic_1 > bic_2)&(bic_2 > bic_3)&(bic_3 > bic_4)&(bic_4 > bic_5)&(bic_5 > bic_6)&(bic_6 > bic_7)&(bic_7 > bic_8)&(bic_8>bic_9)) #YES 
```
```{r}
length(colnames(ds_fin[,-c(2,5,6,9,18,22,23,24)])) ##At this point, we have 19 variables 
```

##3.3 LASSO logistic regression

```{r}
library(glmnet)


y <- ds_fin$Diabetic
x <- data.matrix(ds_fin[,-12])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1, family="binomial")

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda #0.002622828

#produce plot of test MSE by lambda value (optional)
plot(cv_model) 

lasso_reg <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
coef(lasso_reg)

########## LASSO drops very different coefficients than the one we would drop using stepwise selection :(( 
#An idea could be to research what simple logistic and LASSO are used for (i.e. are they used for prediction/classification
# or rather to find out which variables "weigh" more in the insurgence of, say, a sickness?) in order to figure out which one is best. 


```

