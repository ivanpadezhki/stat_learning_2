---
title: "Statistical Learning B: Final Project"
subtitle: "Diabetes Type II Prediction and Classification"
author: "Berke Furkan Kusmenoglu, Ivan Dragomirov Padezhki, Elisa Tremolada"
date: "17/06/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

Import necessary libraries for the analysis.

```{r, label= 'libraries', message = FALSE}
library(tidyverse)
library(MASS)
library(caret)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(Hmisc)
library(corrplot)
library(RColorBrewer)
library(stats)
library(readr)
library(glmnet)
library(gridExtra) 
library(grid)
library(car)
library(ROCR)
```

# 1 Dataset

## 1.1 Introduction

The data set chosen for the current project is related to a paper exploring the connection between lifestyle, family history and Type 2 diabetes in the Indian population. The related article is Tigga, N. P., & Garg, S. (2020). Prediction of Type 2 Diabetes using Machine Learning Classification Methods. Procedia Computer Science, 167, 706-716. DOI: <https://doi.org/10.1016/j.procs.2020.03.336>. The data set can be found at <https://www.kaggle.com/datasets/tigganeha4/diabetes-dataset-2019> and was first accessed on 30/05/2022.

## 1.2 Importing the data

```{r}
library(readr)
ds <- read_csv("https://raw.githubusercontent.com/ivanpadezhki/stat_learning_2/master/data/diabetes_dataset__2019.csv?token=GHSAT0AAAAAABTXCTNAD4HUUN5MUAT2TSDEYU6D7XA")

```

## 1.3 Data preprocessing

From a preliminary inspection we find three variables ('BPLevel', 'Pdiabetes', 'Diabetes') have received different codes for the same levels (e.g. some are lowercase, others start with a capital letter).

Moreover, we report below all levels for each variables, which will be useful to transform them into binary variables.

```{r}
for(i in 1:ncol(ds)) {       
  print(lapply(ds[i], unique))
}
```

### 1.3.1 Normalization of value labels

```{r}
# Copy data into a new variable to retain original data frame
ds_fin <- ds

# Fixing variable name Pregancies
ds_fin <- ds_fin %>% rename(Pregnancies=Pregancies)

#Fixing the variable Pdiabetes 
ds_fin$Pdiabetes[ds_fin$Pdiabetes == ''] <- NA
ds_fin$Pdiabetes[ds_fin$Pdiabetes == ' no'] <- 'no'
ds_fin$Pdiabetes[ds_fin$Pdiabetes == '0'] <- 'no'

#Fixing the variable Diabetic
ds_fin$Diabetic[ds_fin$Diabetic == ' no'] <- 'no'
ds_fin$Diabetic[ds_fin$Diabetic == ''] <- NA

#Fixing the variable BPLevel
ds_fin$BPLevel[ds_fin$BPLevel == 'High'] <- 'high'
ds_fin$BPLevel[ds_fin$BPLevel == 'Low'] <- 'low'
ds_fin$BPLevel[ds_fin$BPLevel == 'normal '] <- 'normal'

# Fixing the variable RegularMedicine
ds_fin$RegularMedicine[ds_fin$RegularMedicine == 'o'] <- 'no'
```

### 1.3.2 Identifying Missing Values

The whole data set contains 48 missing values; in light of the relatively large number of observations (952), the rows containing NA values will be removed from the data set prior to analysis.

```{r}
sum(is.na(ds_fin)==TRUE)

```

First, we investigate the rows with missing values to identify potential patterns.

In fact, most of the missing values are in the variable 'Pregnancies'. The research paper provides no explanation for the high number of NAs in this variable. Furthermore, the NAs are present for both female and male individuals, so there is not a reason to suspect a pattern.

Thus, the rows containing missing values are removed from the data set.

```{r}
ds_fin[rowSums(is.na(ds_fin)) > 0, ]
ds_fin <- na.omit(ds_fin)
```

#### 1.3.2.1 Identifying weird observations

A later check of the data led to the discovery of 12 individuals coded as "Male" who at the same time have a value larger than 0 for the variable Pregnancies. Since the chance of having 12 transgender men who have had 1 or more pregnancies in our data set are rather low, these observations are excluded from further analyses.

```{r}
preg_male <- subset(ds_fin, Gender == 'Male' & Pregnancies != 0)
# Take a closer look at the observations to potentially identify if it is a data entry mistake
preg_male
# exclude individuals from data set
ds_fin <- ds_fin[!(ds_fin$Gender == 'Male' & ds_fin$Pregnancies != 0),]
```

### 1.3.3 Turning categorical variables into dummy variables

Binary variables with yes/no as levels are coded as 1 versus 0.

For variables with more than two levels, the lowest level is taken as baseline and the rest are contrasted with it.

The reason for this choice of dummy coding is that none of the categorical variables are nominally scaled, rather they are ordinal variables without measurable distances between different levels. Therefore, coding them as numerical variables is not appropriate.

The end result is a data set containing 26 predictor variables.

```{r}

# Keep ds_pre for creating labeled plots easier later
ds_pre <- ds_fin


# Variable AGE - has 4 levels, cast into 3 dummy variables with baseline <40 yo
ds_fin$Age40_49 <- ifelse(ds_fin$Age == "40-49", 1, 0)
ds_fin$Age50_59 <- ifelse(ds_fin$Age == "50-59", 1, 0)
ds_fin$Age_60 <- ifelse(ds_fin$Age == "60 or older", 1, 0)
ds_fin$Age <- NULL

# Variable GENDER (has two levels: male/female)
ds_fin$Gender_M <- ifelse(ds_fin$Gender == "Male", 1, 0)
ds_fin$Gender <- NULL

# Variable FAMILY DIABETES - has two levels (yes/no)
ds_fin$Family_Diabetes <- ifelse(ds_fin$Family_Diabetes == "yes", 1, 0)

# Variable high blood pressure - has two levels (yes/no)
ds_fin$highBP <- ifelse(ds_fin$highBP == "yes", 1, 0)

# Variable Physically Active: has 4 levels: none/ less than 30 min/ more than 30 min/ more than 1 hr
# cast into three dummy variables with "none" as baseline
ds_fin$PhysLow <- ifelse(ds_fin$PhysicallyActive == "less than half an hr", 1, 0)
ds_fin$PhysMid <- ifelse(ds_fin$PhysicallyActive == "more than half an hr", 1, 0)
ds_fin$PhysHigh <- ifelse(ds_fin$PhysicallyActive == "one hr or more", 1, 0)
ds_fin$PhysicallyActive <- NULL 

#Vairable BMI: we know from the paper underlying the dataset that having a BMI>25 is considered an 
#important risk factor for developing diabetes. Hence, we turn BMI into a dummy variable,
# taking value=0 when the subject has BMI < 25 and value 1 when BMI >= 25. 

ds_fin$BMI <- ifelse(ds_fin$BMI >= 25, 1, 0)

# Variable Smoking - has two levels (yes/no)
ds_fin$Smoking <- ifelse(ds_fin$Smoking == "yes", 1, 0)

# Variable Alcohol - has two levels (yes/no)
ds_fin$Alcohol <- ifelse(ds_fin$Alcohol == "yes", 1, 0)

# Variable Regular Medicine - has two levels yes/no
ds_fin$RegularMedicine <- ifelse(ds_fin$RegularMedicine == "yes", 1, 0)

# Variable Junk Food - contains 4 levels: occasionally/ often/ very often/ always
ds_fin$JunkOften <- ifelse(ds_fin$JunkFood == "often", 1, 0)
ds_fin$JunkVeryOften <- ifelse(ds_fin$JunkFood == "very often", 1, 0)
ds_fin$JunkAlways <- ifelse(ds_fin$JunkFood == "always", 1, 0)
ds_fin$JunkFood <- NULL

# Variable Stress has 4 levels: not at all/ sometimes/ very often/ always
ds_fin$StressSometimes <- ifelse(ds_fin$Stress == "sometimes", 1, 0)
ds_fin$StressOften <- ifelse(ds_fin$Stress == "very often", 1, 0)
ds_fin$StressAlways <- ifelse(ds_fin$Stress == "always", 1, 0)
ds_fin$Stress <- NULL

# Variable BPLevel - similar to highBP, but has three levels (low/normal/high)
ds_fin$BPNormal <- ifelse(ds_fin$BPLevel == "normal", 1, 0)
ds_fin$BPHigh <- ifelse(ds_fin$BPLevel == "high", 1, 0)
ds_fin$BPLevel <- NULL

# Variable PDiabetes: Gestational Diabetes (during pregnancy) - contains yes/no
ds_fin$Pdiabetes <- ifelse(ds_fin$Pdiabetes == "yes", 1, 0)

# Variable Urination Frequency with two levels - not much/ quite often
ds_fin$UriationFreq <- ifelse(ds_fin$UriationFreq == "quite often", 1, 0)

# Outcome Variable Diabetic with two levels - yes/ no
ds_fin$Diabetic <- ifelse(ds_fin$Diabetic == "yes", 1, 0)
```

Finally, we perform a check of the transformation.

```{r}
# this for-loop returns an output only if a certain variable is not numeric

for(i in 1:ncol(ds_fin)) {       
  if (lapply(ds_fin[1], class) != 'numeric'){
    print(lapply(ds_fin[i], class))
  }
}
```

### 1.3.4 Checking numerical variables

We investigate numerical variables BMI, Sleep and SoundSleep means and standard deviations ['BMI': \mu = 25.764, std = 5.403; 'Sleep' (\mu = 6.957, std = 1.273); 'SoundSleep' (\mu = 5.547, std = 1.871)]. However, we will use logistic regression, so we do not need to standardize these variables.

```{r}
mean(ds_pre$BMI)
sd(ds_pre$BMI)

mean(ds_fin$Sleep)
sd(ds_fin$Sleep)

mean(ds_fin$SoundSleep)
sd(ds_fin$SoundSleep)

```

## 1.4 Exploratory Data Analysis: Graphical EDA

An important question is whether the outcome variable is balanced. In the current data set, our class of interest (diabetic subjects) has less than half the observations with respect to non-diabetic subjects.

Since this is not a large imbalance, the analyses will be performed without any data over/undersampling.

However, the development of this problem will be monitored throughout the analysis, especially in the diagnosis of the model.

```{r}
table(ds_fin$Diabetic)
```

In this section we produce some plots, with the intention of better understanding the distribution of the predictor variables in our dataset - both with respect to each other and with respect to the output variable "Diabetic".

### 1.4.1 Numerical Variables

```{r}
# histogram of numerical variables
hist_bmi <- ggplot(ds_pre, aes(x=BMI)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "BMI")
hist_sleep <- ggplot(ds_fin, aes(x=Sleep)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Hours of Sleep")
hist_soundsl <- ggplot(ds_fin, aes(x=SoundSleep)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Hours of Sound Sleep")
hist_preg <- ggplot(ds_fin[ds_fin$Gender_M!=1,], aes(x=Pregnancies)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Pregnancies (Women)")
grid.arrange(hist_bmi, hist_sleep, hist_soundsl, hist_preg, ncol = 4, top=textGrob("Histograms of Numerical Variables"))
```

Furthermore, the relationship of the numerical variables with the outcome variable is explored. There doesn't seem to be a reason to expect large differences in the outcome variable based on the three numerical variables studied below.

```{r}
box_bmi <- ggplot(ds_pre, aes(x=Diabetic, y=BMI)) + 
    geom_boxplot() +
  geom_jitter(alpha = 0.5, width = 0.2, height = 0.2, color = "tomato")
box_sleep <- ggplot(ds_pre, aes(x=Diabetic, y=Sleep)) + 
    geom_boxplot() + 
  geom_jitter(alpha = 0.5, width = 0.2, height = 0.2, color = "tomato")
box_ssleep <- ggplot(ds_pre, aes(x=Diabetic, y=SoundSleep)) + 
    geom_boxplot() +
  geom_jitter(alpha = 0.5, width = 0.2, height = 0.2, color = "tomato")
grid.arrange(box_bmi, box_sleep, box_ssleep, ncol = 3, top=textGrob("Boxplots of Numerical Variables"))
```

Additionally, the relationship between Sleep and Sound Sleep is evaluated. There seems to be reason to suspect possible interaction between the two variables, potentially due to correlation. For this reason, in a future model we will consider excluding one of the two variables.

```{r, message=FALSE}
ggplot(ds_pre, aes(x=Sleep, y=SoundSleep, shape = Diabetic, color = Diabetic)) + geom_point() +
  geom_smooth(method=lm, level=0.95) + 
  labs(y = "Hours of Sound Sleep", title = "Scatter Plot of Sleep and Sound Sleep") +
  # move the title text to the middle
  theme(plot.title=element_text(hjust=0.5))
```

### 1.4.2 Categorical variables

Next, we inspect the categorical variables and their relationships.

```{r}
get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
```

```{r}
# build plots
bar_age <- ggplot(ds_pre, aes(x=reorder(Age, Age, function(x)-length(x)))) + 
   geom_bar() +  labs(x='Age') + 
  scale_x_discrete(labels=c("less than 40" = "< 40", "40-49" = "40-49",
                              "50-59" = "40-49", "60 or older" = ">60"))
bar_gender <- ggplot(ds, aes(x=reorder(Gender, Gender, function(x)-length(x)))) + 
   geom_bar() +  labs(x='Gender')
bar_familyd <- ggplot(ds, aes(x=reorder(Family_Diabetes, Family_Diabetes, function(x)-length(x)))) + 
   geom_bar() +  labs(x='Family Diabetes')

bar_smoking <- ggplot(data = ds_pre, aes(x = Smoking)) +
    geom_bar() + theme(legend.position = "none")
bar_alc <- ggplot(data = ds_pre, aes(x = Alcohol)) +
    geom_bar() + theme(legend.position = "none")
bar_med <- ggplot(data = ds_pre, aes(x = RegularMedicine)) +
  geom_bar() + theme(legend.position="none")
# reorder Stress levels
ds_pre$Stress <- factor(ds_pre$Stress,levels = c("not at all", "sometimes", "very often", "always"))
# plot Stress
bar_stress <- ggplot(data = ds_pre, aes(x = Stress)) +
  geom_bar() + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  scale_x_discrete(labels=c("not at all" = "none.", "sometimes" = "somet.",
                              "very often" = "v. often", "always" = "alw."))

# reorder levels of JunkFood
ds_pre$JunkFood <- factor(ds_pre$JunkFood,levels = c("occasionally", "often", "very often", "always"))
bar_junk <- ggplot(data = ds_pre, aes(x = JunkFood)) +
    geom_bar() + theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
   scale_x_discrete(labels=c("occasionally" = "occas.", "often" = "often",
                              "very often" = "v. often", "always" = "alw."))

# reorder levels of Physical Activity
ds_pre$PhysicallyActive <- factor(ds_pre$PhysicallyActive,levels = c("none", "less than half an hr", "more than half an hr", "one hr or more"))
# plot physical activity 
bar_act <- ggplot(data = ds_pre, aes(x = PhysicallyActive)) +
    geom_bar() + theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  scale_x_discrete(labels=c("none" = "none", "less than half an hr" = "<30 m",
                              "more than half an hr" = ">30 m", "one hr or more" = ">1 hr"))

bar_urin <- ggplot(data = ds_pre, aes(x = UriationFreq)) +
    geom_bar() + theme(legend.position = "none")
# reorder BPlevel to be low, normal, high
ds_pre$BPLevel <- factor(ds_pre$BPLevel,levels = c("low", "normal", "high"))
# plot BPLevel 
bar_bplev <- ggplot(data = ds_pre, aes(x = BPLevel)) +
    geom_bar() + theme(legend.position="none")
bar_highbp <- ggplot(data = ds_pre, aes(x = highBP)) +
  geom_bar() + theme(legend.position="none")

# Plot first group 
grid.arrange(bar_age, bar_gender, bar_familyd, ncol = 3, top=textGrob("Barplots of First Group of Categorical Variables"))
# plot second group
grid.arrange(bar_smoking, bar_alc, bar_med, bar_urin, bar_bplev, bar_highbp, ncol = 3, top=textGrob("Bar Plots of Categorical Variables 2"))
# plot third group
grid.arrange(bar_junk, bar_act, bar_stress, ncol = 3, top=textGrob("Bar Plots of Categorical Variables 3"))

```

Create proportional plots for relevant variables and their relationship with the outcome variable.

```{r}
# Variable Age
# reorder levels
ds_pre$Age <- factor(ds_pre$Age,levels = c("less than 40", "40-49", "50-59", "60 or older"))
prop_age <- ggplot(data = ds_pre, aes(x = Age, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = 'none') +
  labs(y = 'Proportion') + 
  scale_x_discrete(labels=c("less than 40" = "< 40", "40-49" = "40-49",
                              "50-59" = "40-49", "60 or older" = ">60"))
# Gender
prop_gender <- ggplot(data = ds_pre, aes(x = Gender, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = 'none') +
  labs(y = 'Proportion')
# Family Diabetes
prop_famdiab <- ggplot(data = ds_pre, aes(x = Family_Diabetes, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = 'none') +
  labs(y = 'Proportion')
# Junk Food
prop_junk <- ggplot(data = ds_pre, aes(x = JunkFood, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  labs(y = "Proportion") +
  scale_x_discrete(labels=c("occasionally" = "occas.", "often" = "often",
                              "very often" = "v. often", "always" = "alw."))
# Alcohol
prop_alc <- ggplot(data = ds_pre, aes(x = Alcohol, fill = Diabetic)) +
  geom_bar(position = 'fill') + theme(legend.position = "none") + labs(y = "Proportion")
# Physical Activity
prop_act <- ggplot(data = ds_pre, aes(x = PhysicallyActive, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  labs(y = "Proportion") + 
  scale_x_discrete(labels=c("none" = "none", "less than half an hr" = "<30 m",
                              "more than half an hr" = ">30 m", "one hr or more" = ">1 hr"))
# Regular Medicine
prop_med <- ggplot(data = ds_pre, aes(x = RegularMedicine, fill = Diabetic)) +
  geom_bar(position = 'fill') + theme(legend.position="none") + labs(y = "Proportion")
# Smoking
prop_smoke <- ggplot(data = ds_pre, aes(x = Smoking, fill = Diabetic)) +
  geom_bar(position = 'fill') + theme(legend.position = "none") + labs(y = "Proportion")
# Stress
prop_stress <- ggplot(data = ds_pre, aes(x = Stress, fill = Diabetic)) +
  geom_bar(position = 'fill') + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  labs(y = "Proportion") +
  scale_x_discrete(labels=c("not at all" = "none.", "sometimes" = "somet.",
                              "very often" = "v. often", "always" = "alw."))
# Blood Pressure Level
prop_bplev <- ggplot(data = ds_pre, aes(x = BPLevel, fill = Diabetic)) +
    geom_bar(position = 'fill') + theme(legend.position="none")
# High Blood pressure
prop_highbp <- ggplot(data = ds_pre, aes(x = highBP, fill = Diabetic)) +
  geom_bar(position = 'fill') + theme(legend.position="none")
# Urination Frequency
prop_urin <- ggplot(data = ds_pre, aes(x = UriationFreq, fill = Diabetic)) +
    geom_bar(position = 'fill')
# get legend
legend <- get_legend(prop_urin)
# delete legend from past plot
prop_urin <- prop_urin + theme(legend.position="none")

# Create graph with multiple plots
grid.arrange(prop_age, prop_gender, prop_famdiab, prop_med, legend, ncol = 3, top=textGrob("Bar plots of categorical Variables 1"))
grid.arrange(prop_junk, prop_act, prop_stress, prop_smoke, prop_alc, ncol = 3, top=textGrob("Bar plots of categorical Variables 2"))
grid.arrange(prop_bplev, prop_highbp, prop_urin, legend, ncol = 3, top=textGrob("Bar plots of categorical Variables 3"))
```

### 1.4.3 Further exploration

Then, we take a look at some relationships between categorical and numerical variables

```{r}
gender_f <- ds_pre[ds_pre$Gender=='Female',]
ggplot(gender_f, aes(x=Diabetic, y=Pregnancies, fill = Pdiabetes)) + 
  geom_boxplot() + 
  geom_jitter(alpha = 0.5, width = 0.2, height = 0.2, color = "tomato") + 
  labs(x = "Diabetes", y = "Pregnancies", title = "Diabetes by Gestational Diabetes and Number of Pregnancies")+
  # move the title text to the middle
  theme(plot.title=element_text(hjust=0.5))
```

```{r}

box_alc.bmi <- ggplot(ds_pre, aes(x=Alcohol, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Alcohol Consumption & Diabetes")
box_age.bmi <- ggplot(ds_pre, aes(x=Age, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Age & Diabetes") + 
  scale_x_discrete(labels=c("less than 40" = "< 40", "40-49" = "40-49",
                              "50-59" = "40-49", "60 or older" = ">60"))
box_gender.bmi <- ggplot(ds_pre, aes(x=Gender, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Gender & Diabetes")
box_smoke.bmi <- ggplot(ds_pre, aes(x=Smoking, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Smoking & Diabetes")
box_fam.bmi <- ggplot(ds_pre, aes(x=Family_Diabetes, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Family Diabetes & Diabetes")
box_highbp.bmi <- ggplot(ds_pre, aes(x=highBP, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by High BP & Diabetes")
box_med.bmi <- ggplot(ds_pre, aes(x=RegularMedicine, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Regular Medicine and Diabetes")
box_urin.bmi <- ggplot(ds_pre, aes(x=UriationFreq, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Urination Frequency and Diabetes")
box_pdiab.bmi <- ggplot(gender_f, aes(x=Pdiabetes, y=BMI, fill = Diabetic)) + 
    geom_boxplot() + labs(title = "by Gestational Diabetes and Diabetes")

grid.arrange(box_age.bmi, box_gender.bmi, box_fam.bmi, 
             ncol = 3, top=textGrob("Box Plots of BMI and Categorical Variables"))
grid.arrange(box_med.bmi, box_alc.bmi, box_smoke.bmi,
             ncol = 3, top=textGrob("Box Plots of BMI and Categorical Variables"))
grid.arrange(box_highbp.bmi, box_urin.bmi, box_pdiab.bmi, 
             ncol = 3, top=textGrob("Box Plots of BMI and Categorical Variables"))

```

### 1.4.4 Correlation matrix

In order to assess which variables should be kept when modeling our data, we produce an overview of the correlation structure of all variables

```{r}

chisq.test(ds_pre$BPLevel, ds_pre$highBP)

```
We conclude that the two variables concerning blood pressure are not independent, thus potentially we would remove one of them from the analyses. 

Additionally, we check the relationship between Smoking and Alcohol, which also result in the Chi-squared coefficient to be significant. 

```{r}
chisq.test(ds_pre$Smoking, ds_pre$Alcohol)
```
Next couple of variables to be tested are Regular Medicine and the Outcome variable Diabetic. In addition to the concern that the data set contains individuals who already have been diagnosed with Diabetes, we have the fact that the two variables are not independent. Therefore, we will consider excluding Regular Medicine from the models. 

```{r}
chisq.test(ds_pre$RegularMedicine, ds_pre$Diabetic)
```
Furthermore, we investigate the relationship between Sleep and Sound Sleep, which potentially carry similar information as seen in the plot above. 
```{r}
chisq.test(ds_pre$Sleep, ds_pre$SoundSleep)
```
With a better idea of the structure of the variables, we move on to the modelling of the data. 

# 2 Modeling

First of all, we perform a train-test split of the dataset, in order to be able to carry out model validation later. 

```{r}
# Split the data into training and test set

set.seed(123)
training.samples <- ds_fin$Diabetic %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- ds_fin[training.samples, ]
test.data <- ds_fin[-training.samples, ]
```

## 2.1 Logistic Regression

Following the rationale of the paper from which the dataset is taken, we wish to create a model which is able to satisfy two objectives:

1)  Identify the lifestyle and family factors which increase / decrease the probability of developing Type 2 diabetes
2)  Classify, as correctly as possible, diabetic and non-diabetic subjects

We believe in this case it is important to choose an interpretable model, since this analysis is mainly aimed at preventing the insurgence of diabetes; while models such as KNN could reach better performance in the classification task, they would provide no indication of which factors "weigh" more in the development of this pathology.

Thus, we choose to carry out a logistic regression on the binary variable "Diabetic". The full model is run below.

```{r}
# Full model 
log_reg <- glm(Diabetic ~., data = train.data, family = binomial)
summary(log_reg)

#Saving the AIC for later use

AIC_full = 432.18
```
In addition, we remove RegularMedicine from the list of predictors, due to concerns that it is actually a result from being diagnosed with Diabetes instead of a predictor of it. Simply observing the significance of the coefficients of the predictors, we now have a larger number of significant predictors and a larger AIC. This could indicate that havign Regular Medicien in the model acts as a suppressor variable. 
```{r}
# Full model 
log_reg_nomed <- glm(Diabetic ~.-RegularMedicine, data = train.data, family = binomial)
summary(log_reg_nomed)

aic_nomed <- 514.91
```

## 2.2 Model diagnostics

Now, we use the first full model for prediction in order to assess its classification accuracy.

We are particularly interested in how well the model is able to identify diabetic subjects; since these results will likely be used for identifying good prevention practices rather than as a diagnostic tool (and, even then, the diagnosis would have to be confirmed through clinical test), we are not particularly worried about identifying a patient as sick when she is not - while we do care about identifying all truly sick patients as sick.

Hence, we are interested in calculating the sensitivity and precision of our model, which are reported below.

```{r}
#Prediction and confusion matrix for full model 
dim(test.data)
log_prob <- predict(log_reg, test.data, type = 'response')
logistic_pred <- rep(0, dim(test.data)[1])
logistic_pred[log_prob>0.5] <- 1
full_confusion <- confusionMatrix(data=as.factor(logistic_pred), reference = as.factor(test.data$Diabetic), positive = '1')
full_confusion
```
Compared to the full mode, the one without Regular Medicine has lower accuracy, however we still keep the variable excluded. 
```{r}
#Prediction and confusion matrix for model without Regular Medicine
dim(test.data)
log_prob_nomed <- predict(log_reg_nomed, test.data, type = 'response')
logistic_pred <- rep(0, dim(test.data)[1])
logistic_pred[log_prob_nomed>0.5] <- 1
nomed_confusion <- confusionMatrix(data=as.factor(logistic_pred), reference = as.factor(test.data$Diabetic), positive = '1')
nomed_confusion
```


Moving forward with the model without RegularMeducine, We try to visualize the best threshold for the probability in logistic regression - it might not be the usual 0.5 threshold.

```{r}
full_prediction <- prediction(logistic_pred, test.data$Diabetic)

roc_curve_full <- performance(full_prediction, measure="tpr", x.measure = "fpr" )

auc <- performance(full_prediction, measure="auc")

auc <- auc@y.values[[1]]

auc <- as.character(round(auc, 3))

plot(roc_curve_full, lwd = 2, main = "Full model ROC curve")

full_auc = paste("AUC = ", auc)
text(0.5, 0.5, full_auc)

```

From the graph, it is clear our model does not behave well with respect to the trade-off between sensitivity and specificity (in order to maximize sensitivity here, we would need to classify all subejcts as diabetic!). We proceed to look for the best model and will repeat this procedure every time a new promising model is found.

## 2.3 Variable selection

### 2.3.1 Diagnostics of first model: VIF analysis and multicollinearity check

Now, we check multicollinearity by checking the VIF for all coefficients in the logistic regression.

```{r}
# Vector of VIF values 

all_vif <- vif(log_reg_nomed)
length(all_vif)

# Create bar chart to display VIF values 

barplot(all_vif, main = "VIF for all coefficients - FULL MODEL", horiz = FALSE, col = "orange", names.arg = c('Fam_d','h_bp','BMI','smoke','alc','sleep','Ssleep','preg','pdiab','ur','40-49','50-59','60ab','male','lowphy','midphy','highphy','junkof','junkvof','junkal','stress1','stress2','stress3','normbp','highbp'), ylim = c(0,30)) 

# Add a line to highlight severe correlation when VIF > 5 

abline(h = 5, lwd = 3, lty = 1)    

```

We can observe from the graph above that VIF is under the critical value of 5 for all coefficient except for the ones related to the variables "BPNormal" and "BPHigh". Next, due to the chi-squared test above and the inflated variance, we consider a model without one of the variables.

First, we produce a model without "BPNormal". 

```{r}
# 1. Model without variables: "BPNormal"

log_reg1 <- glm(Diabetic ~. -RegularMedicine-BPNormal, data = train.data, family = binomial)
summary(log_reg1)

#Saving the AIC for later use

AIC_1 = 515.43
```

Secondly, we produce a model without "BPHigh".

```{r}

# 2. Model without variables: "BPHigh"

log_reg2 <- glm(Diabetic ~. -RegularMedicine-BPHigh, data = train.data, family = binomial)
summary(log_reg2)

#Saving the AIC for later use

AIC_2 = 517.78
```

Below, we confirm both through AIC analysis and through the VIF plot for all coefficients that, indeed, the variable BPNormal suffered from multicollinearity and shall be removed from the model. Once removed, all coefficients show VIFs under the critical value of 5.

```{r}
#Checking which variable dropped produced a lowering of the AIC 

AIC_full > AIC_1 #TRUE

AIC_full > AIC_2 #FALSE 

# Creating a new VIF plot for log_reg1 (without BPNormal) 

all_vif1 <- vif(log_reg1)
length(all_vif1)

# Create bar chart to display VIF values 

barplot(all_vif1, main = "VIF for all coeficients - MODEL WITHOUT BPNormal", horiz = FALSE, col = "orange", names.arg = c('Fam_d','h_bp','BMI','smoke','alc','sleep','Ssleep','preg','pdiab','ur','40-49','50-59','60ab','male','lowphy','midphy','highphy','junkof','junkvof','junkal','stress1','stress2','stress3','highbp'), ylim = c(0,30)) 

# Add a line to highlight severe correlation when VIF > 5 

abline(h = 5, lwd = 3, lty = 1)    

```

Now, we observe this new model's performance through VIF analysis, sensitivity and precision measures.

```{r}
#Prediction and confusion matrix for model identified through VIF analysis

log_prob_VIFmod  <- predict(log_reg1, test.data, type = 'response')
logistic_pred_VIFmod  <- rep(0, dim(test.data)[1])
logistic_pred_VIFmod [log_prob_VIFmod>0.5] <- 1
vif_confusion <- confusionMatrix(data=as.factor(logistic_pred_VIFmod), reference = as.factor(test.data$Diabetic), positive = '1')
vif_confusion
```

Now, we check for the best treshold of P in the case of the new model without BPNormal (log_reg1).

```{r}
VIFmod_prediction <- prediction(logistic_pred_VIFmod, test.data$Diabetic)

roc_curve_VIFmod <- performance(VIFmod_prediction, measure="tpr", x.measure = "fpr" )

auc <- performance(VIFmod_prediction, measure="auc")

auc <- auc@y.values[[1]]

auc <- as.character(round(auc, 3))

plot(roc_curve_VIFmod, main="ROC curve - Model modified through VIF analysis", lwd=2)

full_auc = paste("AUC = ", auc)

text(0.5, 0.5, full_auc)


```

### 2.3.2 Stepwise logistic regression

Even after observing the VIFs and dropping BPNormal, we still have 25 explanatory variables in our model, many of which are not significant. In order to explore the possibility of dropping other variables, we perform an automatic stepwise selection procedure.

```{r}
#Stepwise logistic

model <- glm(Diabetic ~.-RegularMedicine, data = train.data, family = binomial)
summary(model)
step.model <- model %>% stepAIC(trace = FALSE)
summary(step.model)
```

Now, we save the model with best AIC.

```{r}
#Saving the model with best AIC 

best.aic <- step.model
summary(best.aic)

#Saving the AIC 
best_AIC = 426.4
```

Finally, we produce the confusion matrix and sensitivity + precision measure for the best AIC model in order to evaluate its performance. 

```{r}

#Prediction and confusion matrix for best AIC model 

log_prob_bestAIC <- predict(best.aic, test.data, type = 'response')
logistic_pred_bestAIC <- rep(0, dim(test.data)[1])
logistic_pred_bestAIC[log_prob_bestAIC>0.5] <- 1
aic_confusion <- confusionMatrix(data=as.factor(logistic_pred_bestAIC), reference = as.factor(test.data$Diabetic), positive = '1')
aic_confusion

```

```{r}
best_aic_prediction <- prediction(logistic_pred_bestAIC, test.data$Diabetic)

roc_curve_best_Aic <- performance(best_aic_prediction, measure="tpr", x.measure = "fpr" )

auc <- performance(best_aic_prediction, measure="auc")

auc <- auc@y.values[[1]]

auc <- round(auc, 3)

plot(roc_curve_best_Aic, main="ROC curve - Model with best AIC", lwd=2)

full_auc = paste("AUC = ", auc)

text(0.5, 0.5, full_auc)

```

### 2.3.3 Comparison of models

In order to check which is our best model, we visually contrast both new models with the full model predictions in order to select the model with best performance.

```{r}

plot(roc_curve_full, col='orange', main='ROC curves for all models', lty=1, lwd=2)

plot(roc_curve_VIFmod, add=TRUE, col='green', lty=3, lwd=2)

plot(roc_curve_best_Aic, add=TRUE, col='blue', lty=2, lwd=2)

auc <- performance(best_aic_prediction, measure="auc")

auc <- auc@y.values[[1]]

auc <- round(auc, 3)

bestAIC_auc = paste("best model AUC = ", auc)

text(0.15, 0.3, full_auc, col='blue')


legend(0.5, 0.5, legend=c("Full model", "VIF modified model", "Best AIC model"),
       col=c("orange", "green", "blue"), lty=c(1,3,2))

```