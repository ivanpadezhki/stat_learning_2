---
title: "FINAL_stat"
author: "ET"
date: "06/06/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
Import necessary libraries for the analysis. 
```{r, label= 'libraries', include=FALSE}
library(tidyverse)
library(MASS)
library(caret)
library(smotefamily)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(Hmisc)
library(corrplot)
library(RColorBrewer)
library(stats)
library(readr)
library(glmnet)
library(gridExtra) 
library(grid)
```

# 1. Input of data

## 1.1. Dataset Breast Cancer
The data set chosen for the current project concerns a study on Diabetes. The related article is Tigga, N. P., & Garg, S. (2020). Prediction of Type 2 Diabetes using Machine Learning Classification Methods. Procedia Computer Science, 167, 706-716. DOI: https://doi.org/10.1016/j.procs.2020.03.336 and the data set can be found at <https://www.kaggle.com/datasets/tigganeha4/diabetes-dataset-2019>, which was first accessed on 30/05/2022.


Read in data directly from GitHub
```{r}
library(readr)
ds <- read_csv("https://raw.githubusercontent.com/ivanpadezhki/stat_learning_2/master/data/diabetes_dataset__2019.csv?token=GHSAT0AAAAAABTXCTNAD4HUUN5MUAT2TSDEYU6D7XA")

```
The data set contains 18 columns - 17 predictor variables and one outcome variable. After taking an initial look at the structure of the data frame, it is visible that most of the variables are categorical and only few predictors have numerical values. The outcome variable is binary and describes whether the individual observation is a case of diabetes or not. 
```{r}
ds[1:5,]
```

Furthermore, an inspection of the unique values for all variables shows that some categorical variables (BPLevel, Pdiabetes, Diabetes) have received different codes for the same levels (e.g. some are lowercase, others start with a capital letter or for example some have spaces). On the other hand, the numerical variables are BMI, Sleep and SoundSleep. BMI is ranging from 15 to 45 with a mean of 25.764 and standard deviation 5.403. The variable Sleep measures the number of hour per night, and ranges from 4 to 11 hours (\mu = 6.95, std = 1.273). The last numerical variable is SoundSleep, which is in a range between 0 and 11 hours per night (mean = 5.496, standard deviation = 1.866). 
```{r}
for(i in 1:ncol(ds)) {       # for-loop over columns
  print(lapply(ds[i], unique))
}
```

The next step in the analysis is to clean the variables. This includes the normalization of the value labels for some variables, as well as the transformation of categorical variables into dummies (e.g. for "Age", base-level will be "less than 40", then we will have 3 dummies for each of the levels specified). 

# 2. Data cleaning

## 2.1. Normalizing value names 

```{r}
# Copy data into a new variable to retain original data frame
ds_fin <- ds

# Fixing variable name Pregancies
ds_fin <- ds_fin %>% rename(Pregnancies=Pregancies)

#Fixing the variable Pdiabetes 
ds_fin$Pdiabetes[ds_fin$Pdiabetes == ''] <- NA
ds_fin$Pdiabetes[ds_fin$Pdiabetes == ' no'] <- 'no'
ds_fin$Pdiabetes[ds_fin$Pdiabetes == '0'] <- 'no'

#Fixing the variable Diabetic
ds_fin$Diabetic[ds_fin$Diabetic == ' no'] <- 'no'
ds_fin$Diabetic[ds_fin$Diabetic == ''] <- NA

#Fixing the variable BPLevel
ds_fin$BPLevel[ds_fin$BPLevel == 'High'] <- 'high'
ds_fin$BPLevel[ds_fin$BPLevel == 'Low'] <- 'low'
ds_fin$BPLevel[ds_fin$BPLevel == 'normal '] <- 'normal'

# Fixing the variable RegularMedicine
ds_fin$RegularMedicine[ds_fin$RegularMedicine == 'o'] <- 'no'
```

## 2.2. Identifying Missing Values

The whole data set contains 48 missing values and in light of the relatively large number of observations, the rows containing NA values will be removed from the data set prior to analysis. 
```{r}
sum(is.na(ds_fin)==TRUE)
```
Before removing the rows, we investigate the cases with missing values to identify potential patterns. In fact, most of the missing values are in the variable 'Pregnancies'. The research paper provides no explanation for the high number of NA's in this variable. Furthermore, the NA's are present for both female and male individuals, so there is not a reason to suspect a pattern. 
```{r}
ds_fin[rowSums(is.na(ds_fin)) > 0, ]
```

Next, the rows containing missing values are removed from the data set. 
```{r}
ds_fin <- na.omit(ds_fin)
ds_pre <- ds_fin
```
However, besides two cases of NAs in the Pregnancies variable, all individuals with missing values belong to the group without diabetes. The following check shows not only that the output variable is unbalanced (this is discussed in detail further below), but also that most removed observations belong to the group without diabetes.

```{r}
table(ds$Diabetic) - table(ds_fin$Diabetic)
```

## 2.3.  Turning categorical variables into single-level dummies

An important step of the preparation of the data for following analyses is the transformation of categorical variables into dummies. Binary variables with yes/no as levels are coded as 1 versus 0. For variables with more than two levels, the lowest level is taken as baseline and the rest are contrasted with it. The reason for this choice of dummy coding is that none of the categorical variables are nominally scaled, rather they are ordinal variables without measurable distances between different levels. Therefore, coding them as numerical variables is not appropriate. The end result is a data set containing 26 predictor variables. 

```{r}
# Variable AGE - has 4 levels, cast into 3 dummy variables with baseline <40 yo
ds_fin$Age40_49 <- ifelse(ds_fin$Age == "40-49", 1, 0)
ds_fin$Age50_59 <- ifelse(ds_fin$Age == "50-59", 1, 0)
ds_fin$Age_60 <- ifelse(ds_fin$Age == "60 or older", 1, 0)
ds_fin$Age <- NULL

# Variable GENDER (has two levels: male/female)
ds_fin$Gender_M <- ifelse(ds_fin$Gender == "Male", 1, 0)
ds_fin$Gender <- NULL

# Variable FAMILY DIABETES - has two levels (yes/no)
ds_fin$Family_Diabetes <- ifelse(ds_fin$Family_Diabetes == "yes", 1, 0)

# Variable high blood pressure - has two levels (yes/no)
ds_fin$highBP <- ifelse(ds_fin$highBP == "yes", 1, 0)

# Variable Physically Active: has 4 levels: none/ less than 30 min/ more than 30 min/ more than 1 hr
# cast into three dummy variables with "none" as baseline
ds_fin$PhysLow <- ifelse(ds_fin$PhysicallyActive == "less than half an hr", 1, 0)
ds_fin$PhysMid <- ifelse(ds_fin$PhysicallyActive == "more than half an hr", 1, 0)
ds_fin$PhysHigh <- ifelse(ds_fin$PhysicallyActive == "one hr or more", 1, 0)
ds_fin$PhysicallyActive <- NULL 

# Variable Smoking - has two levels (yes/no)
ds_fin$Smoking <- ifelse(ds_fin$Smoking == "yes", 1, 0)

# Variable Alcohol - has two levels (yes/no)
ds_fin$Alcohol <- ifelse(ds_fin$Alcohol == "yes", 1, 0)

# Variable Regular Medicine - has two levels yes/no
ds_fin$RegularMedicine <- ifelse(ds_fin$RegularMedicine == "yes", 1, 0)

# Variable Junk Food - contains 4 levels: occasionally/ often/ very often/ always
ds_fin$JunkOften <- ifelse(ds_fin$JunkFood == "often", 1, 0)
ds_fin$JunkVeryOften <- ifelse(ds_fin$JunkFood == "very often", 1, 0)
ds_fin$JunkAlways <- ifelse(ds_fin$JunkFood == "always", 1, 0)
ds_fin$JunkFood <- NULL

# Variable Stress has 4 levels: not at all/ sometimes/ very often/ always
ds_fin$StressSometimes <- ifelse(ds_fin$Stress == "sometimes", 1, 0)
ds_fin$StressOften <- ifelse(ds_fin$Stress == "very often", 1, 0)
ds_fin$StressAlways <- ifelse(ds_fin$Stress == "always", 1, 0)
ds_fin$Stress <- NULL

# Variable BPLevel - similar to highBP, but has three levels (low/normal/high)
ds_fin$BPNormal <- ifelse(ds_fin$BPLevel == "normal", 1, 0)
ds_fin$BPHigh <- ifelse(ds_fin$BPLevel == "high", 1, 0)
ds_fin$BPLevel <- NULL

# Variable PDiabetes: Gestational Diabetes (during pregnancy) - contains yes/no
ds_fin$Pdiabetes <- ifelse(ds_fin$Pdiabetes == "yes", 1, 0)

# Variable Urination Frequency with two levels - not much/ quite often
ds_fin$UriationFreq <- ifelse(ds_fin$UriationFreq == "quite often", 1, 0)

# Outcome Variable Diabetic with two levels - yes/ no
ds_fin$Diabetic <- ifelse(ds_fin$Diabetic == "yes", 1, 0)
```

Perform a check of the transformation. 
```{r}
# returns an output only if a certain variable is not numeric
for(i in 1:ncol(ds_fin)) {       # for-loop over columns
  if (lapply(ds_fin[1], class) != 'numeric'){
    print(lapply(ds_fin[i], class))
  }
}
```

An important question is whether the outcome variable is balanced. In the current data set, there are more than two times more individuals do not have diabetes in the current data set, meaning that people with diabetes are a minority class. Since this is not a large imbalance, the analyses will be performed without any data up/ or down-sampling. However, due to the fact that logistic regression is not well fit for imbalanced data, the development of this problem will be monitored throughout the analysis, especially in the diagnosis of the model.

```{r}
table(ds_fin$Diabetic)
```
# 3. Data Exploration

```{r}
# ggplot(ds, aes(x=reorder(Age, Age, function(x)-length(x)))) + 
#   geom_bar(fill='red') +  labs(x='Age')
# ggplot(ds, aes(x=reorder(Gender, Gender, function(x)-length(x)))) + 
#   geom_bar(fill='green') +  labs(x='Gender')
# ggplot(ds, aes(x=reorder(Family_Diabetes, Family_Diabetes, function(x)-length(x)))) + 
#   geom_bar(fill='green') +  labs(x='Family Diabetes')
# 
# ggplot(ds, aes(x=reorder(highBP, highBP, function(x)-length(x)))) + 
#   geom_bar(fill='blue') +  labs(x='High Blood Pressure')
# ggplot(ds, aes(x=reorder(BPLevel, BPLevel, function(x)-length(x)))) + 
#   geom_bar(fill='blue') +  labs(x='Blood Pressure Level')
```

First, the distributions of the numerical variables are evaluated

```{r}
# histogram of numerical variables
hist_bmi <- ggplot(ds_fin, aes(x=BMI)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "BMI")
hist_sleep <- ggplot(ds_fin, aes(x=Sleep)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Hours of Sleep")
hist_soundsl <- ggplot(ds_fin, aes(x=SoundSleep)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Hours of Sound Sleep")
hist_preg <- ggplot(ds_fin, aes(x=Pregnancies)) + 
  geom_histogram(color="black", fill="white", binwidth = 0.9) + labs(x = "Number of Pregnancies")
grid.arrange(hist_bmi, hist_sleep, hist_soundsl, hist_preg, ncol = 4, top=textGrob("Histograms of Numerical Variables"))
```

Additionally, the relationship between Sleep and Sound Sleep is evaluated. There seems to be reason to suspect possible interaction between the two variables, potentially due to correlation. For this reason, in a future model we will consider excluding one of the two variables. 
```{r}
ggplot(ds_pre, aes(x=Sleep, y=SoundSleep, shape = Diabetic, color = Diabetic)) + geom_point() +
  geom_smooth(method=lm, level=0.95) + labs(y = "Hours of Sound Sleep", title = "Scatter Plot of Sleep and Sound Sleep")
```

Furthermore, the relationship of the numerical variables with the outcome variable is explored. There doesn't seem to be a reason to expect large differences in the outcome variable based on the three numerical variables studied below. 
```{r}
#Is BMI connected to diabetes? 
box_bmi <- ggplot(ds_pre, aes(x=Diabetic, y=BMI)) + 
    geom_boxplot()
box_sleep <- ggplot(ds_pre, aes(x=Diabetic, y=Sleep)) + 
    geom_boxplot()
box_ssleep <- ggplot(ds_pre, aes(x=Diabetic, y=SoundSleep)) + 
    geom_boxplot()
grid.arrange(box_bmi, box_sleep, box_ssleep, ncol = 3, top=textGrob("Boxplots of Numerical Variables"))
```
And the relationship between Diabetes, Pregnancies and Gestational Diabetes is inspected. It seems like the number of pregnancies increases the chance of Gestational Diabetes. 
```{r}
ggplot(ds_pre, aes(x=Pdiabetes, y=Pregnancies, fill = Diabetic)) + 
    geom_boxplot() + labs(x = "Gestational Diabetes", y = "Pregnancies", title = "Number of Pregnancies by Diebetes and Gestational Diabetes")
```

GUYS, I am not sure if we need this deep dive into BMI?? 

```{r}

ggplot(ds_pre, aes(x=Alcohol, y=BMI, fill = Diabetic)) + 
    geom_boxplot()

# BMI and Alcohol
boxplot(BMI~Alcohol, data = ds_fin, main = "BMI of alcohol and non-alcohol subjects",ylab = "BMI",names = c("No Alcohol", "Alcohol"))

# BMI and Gender
boxplot(BMI~Gender_M, data = ds_fin, main = "BMI of Male and Female subjects",ylab = "BMI",names = c("Female", "Male"))

# BMI and Smoking
boxplot(BMI~Smoking, data = ds_fin, main = "BMI of Smoker and Non-Smoker subjects",ylab = "BMI",names = c("Non-Smoker", "Smoker"))

boxplot(BMI~Family_Diabetes, data = ds_fin, main = "BMI of Family_Diabetes subjects",ylab = "BMI",names = c("Have Not", "Have"))

boxplot(BMI~highBP, data = ds_fin, main = "BMI of High Blood-Pressure subjects",ylab = "BMI",names = c("No", "Yes"))

boxplot(BMI~RegularMedicine, data = ds_fin, main = "BMI of Regular Medicine Usage subjects",ylab = "BMI",names = c("No", "Yes"))

boxplot(BMI~UriationFreq, data = ds_fin, main = "BMI of Uriation Frequency subjects",ylab = "BMI",names = c("No", "Yes"))

boxplot(BMI~Pdiabetes, data = ds_fin, main = "BMI of P-Diabetes subjects",ylab = "BMI",names = c("No", "Yes"))

```

I'm not so sure what it happening here - we need to concentrate on a subset of this and present it as one graph potentially. 
```{r}
gender_df <- data.frame(table(ds_fin$Diabetic,ds_fin$Gender_M))
names(gender_df) <- c("Diabetic","Gender","Count")

ggplot(data=gender_df, aes(x=Diabetic, y=Count, fill=Gender, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

alcohol_df <- data.frame(table(ds_fin$Diabetic,ds_fin$Alcohol))
names(alcohol_df) <- c("Diabetic","Alcohol","Count")

ggplot(data=alcohol_df, aes(x=Diabetic, y=Count, fill=Alcohol, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

smoking_df <- data.frame(table(ds_fin$Diabetic,ds_fin$Smoking))
names(smoking_df) <- c("Diabetic","Smoking","Count")

ggplot(data=smoking_df, aes(x=Diabetic, y=Count, fill=Smoking, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

family_df <- data.frame(table(ds_fin$Diabetic,ds_fin$Family_Diabetes))
names(family_df) <- c("Diabetic","FamilyDiabetes","Count")

ggplot(data=family_df, aes(x=Diabetic, y=Count, fill=FamilyDiabetes, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

highBP_df <- data.frame(table(ds_fin$Diabetic,ds_fin$highBP))
names(highBP_df) <- c("Diabetic","HighBP","Count")

ggplot(data=highBP_df, aes(x=Diabetic, y=Count, fill=HighBP, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

RegularMedicine_df <- data.frame(table(ds_fin$Diabetic,ds_fin$RegularMedicine))
names(RegularMedicine_df) <- c("Diabetic","RegularMedicine","Count")

ggplot(data=RegularMedicine_df, aes(x=Diabetic, y=Count, fill=RegularMedicine, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

UriationFreq_df <- data.frame(table(ds_fin$Diabetic,ds_fin$UriationFreq))
names(UriationFreq_df) <- c("Diabetic","UriationFreq","Count")

ggplot(data=UriationFreq_df, aes(x=Diabetic, y=Count, fill=UriationFreq, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))

Pdiabetes_df <- data.frame(table(ds_fin$Diabetic,ds_fin$Pdiabetes))
names(Pdiabetes_df) <- c("Diabetic","Pdiabetes","Count")

ggplot(data=Pdiabetes_df, aes(x=Diabetic, y=Count, fill=Pdiabetes, label = Count)) + geom_bar(stat="identity") + geom_text(size = 3, position = position_stack(vjust = 0.5))
```


Look at an overview of the correlation structure of all variables
```{r}
corr <- cor(ds_fin)
#prepare to drop duplicates    
corr[lower.tri(corr,diag=TRUE)] <- NA 
#plot correlations visually
corrplot(corr, is.corr=FALSE, tl.col="black", na.label=" ")
```
This correlation plot is in fact better, we also have access to the currelation coefficients. 
```{r}
library(ggcorrplot)
model.matrix(~0+., data=ds_fin) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="upper", lab=TRUE, lab_size=2)
```

```{r}
w <- which(abs(corr)>0.5 & row(corr)<col(corr), arr.ind = TRUE)
# reconstruct names from positions
high_cor <- matrix(colnames(corr)[w],ncol=2)
high_cor
```
There are three variables highBP and BPnormal and BPhigh, which carry similar information - we will later check which one to exclude from the analysis (the correlation is above .6)

Additionally, Smoking & Alcohol, Sleep & SoundSleep, StressSometimes & StressOften are correlated above .5. 

Potentially consider oversampling the minority class (Diabetic people) to deal with the balance problem.


```{r}
list(names(ds_fin))
```

# 4. Modeling the data: LOGISTIC REGRESSION 


Performing a train-test split for model validation
```{r}
# Split the data into training and test set
set.seed(123)
training.samples <- ds_fin$Diabetic %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- ds_fin[training.samples, ]
test.data <- ds_fin[-training.samples, ]
```


## 4.1. Full model 
```{r}
log_reg <- glm(Diabetic ~., data = ds_fin, family = binomial)
summary(log_reg)
```


## 4.2. Diagnostics 

```{r}
log_prob <- predict(log_reg, type = 'response')
logistic_pred <- rep(0, 905)
logistic_pred[log_prob>0.5] <- 1
table(logistic_pred, ds_fin$Diabetic)
```
## 4.3. Variable selection 

We try dropping the variable "Sleep" since it is heavily correlated (0.54) with the variable "Sound Sleep" AND its P-value in the logistic regression is close to 1. 

```{r}

log_reg_noSleep <- glm(Diabetic ~. -Sleep, data = ds_fin, family = binomial)
summary(log_reg_noSleep)

## AIC drops to 536.77, CONFIRM dropping variable Sleep  
```
After removing variable Sleep, the variable SounSleep increases in significance. 

Now we produce a new correlation matrix with the new dataset 

```{r}

model.matrix(~0+., data=ds_fin[,-6]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

Now we can observe that the variables Smoking and Alcohol are heavily correlated (0.52) AND Alcohol is not significant 
in the regression with p-value =0.27. Hence, we try dropping the variable Alcohol. 
However, the variable Smoking becomes less significant once Alcohol is dropped, potentially due to variance suppresion.
```{r}
log_reg_noSleep_noAlcohol <- glm(Diabetic ~. -Sleep-Alcohol, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol)

## AIC drops to 536.01: CONFIRM dropping variable Alcohol 
```

```{r}

model.matrix(~0+., data=ds_fin[,-c(5,6)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

Now, we can observe highBP remains non-significant in all models tried so far (p-value=0.24) AND it is heavily correlated to 
BPNormal (-0.64). We attempt two regressions, one dropping highBP and the other one dropping BPNormal in order to understand 
which variable to keep. 

```{r}
log_reg_noSleep_noAlcohol_noBPNormal <- glm(Diabetic ~. -Sleep-Alcohol-BPNormal, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_noBPNormal)
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP <- glm(Diabetic ~. -Sleep-Alcohol-highBP, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP)

## AIC drops to 535.29: CONFIRM dropping highBP, keep BPNormal bc AIC higher for previous regression
```
```{r}

model.matrix(~0+., data=ds_fin[,-c(2,5,6)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now we observe that variable PhysMid is heavily correlated with variable PhysLow AND variable PhysMid is not significant (p-value=0.22). Hence, we try dropping this variable. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid)

## AIC drops to 534.78: CONFIRM dropping PhysMid
```
```{r}

model.matrix(~0+., data=ds_fin[,-c(2,5,6,18)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

```
Now, since the not significant variables StressOften (p-value=0.47) and StressSometimes(p-value=0.38) are heavily correlated (-0.55), we run two models: one without one, and one without the other, to figure out which one should be dropped. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften)

##AIC drops to 533.31: CONFIRM dropping the variable StressOften since the other AIC is lower 
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressSometimes <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressSometimes, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressSometimes)

model.matrix(~0+., data=ds_fin[,-c(2,5,6,18,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we observe the non-significant variables StressAlways (p-value=0.211927) and StressSometimes (p-value=0.612708) are heavily correlated (-0.39).we run two models: one without one, and one without the other, to figure out which one should be dropped.

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Always <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Always)
```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes)

##AIC drops to 531.57: CONFIRM dropping variable StressSometimes
```
```{r}

model.matrix(~0+., data=ds_fin[,-c(2,5,6,18,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Moreover, the variable Pregancies (i.e. n. of pregnancies) is misleading our model since it is heavily correlated to the Male variable(-0.5) for obvious reasons. Since papers on the subejct (FIND A SPECIFIC ONE) point to gestational diabetes as an important factor in order to develop diabetes type 2, rather than pregnancy per se, we try dropping this variable. 

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregnancies, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg)

##AIC drops to 531.02: CONFIRM dropping variable Pregancies 
```

```{r}
model.matrix(~0+., data=ds_fin[,-c(2,5,6,9,18,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we can observe variables JunkAlways and StressAlways are heavily correlated (0.28) AND the variable JunkAlways is not significant (p-value=0.63), while STressAlways is somewhat significant. We try dropping both and observe the AIC. 
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregnancies-JunkAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways)

## AIC drops to 529.25: CONFIRM dropping variable JunkAlways 
```

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregnancies-StressAlways, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways) 
##NO, AIC gets higher 

```
```{r}
model.matrix(~0+., data=ds_fin[,-c(2,5,6,9,18,22,23,24)]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
Now, we see that variable BPLow is not significant AND is heavily correlated with BPNormal (-0.29). We try dropping both and observe the AIC.

```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPlow <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregnancies-StressAlways-BPLow, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPlow) ##NO, AIC gets higher 

```
```{r}
log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPNormal <- glm(Diabetic ~. -Sleep-Alcohol-highBP-PhysMid-StressOften-StressSometimes-Pregnancies-StressAlways-BPNormal, data = ds_fin, family = binomial)
summary(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noStressAlways_noBPNormal) ##NO, AIC gets higher
```
In both cases, AIC gets higher so we DO NOT CONFIRM dropping these variables. 

## 4.4. NB: 
Since AIC works well asymptotically (i.e. when we have many more observations than variables) and our models our quite large, I now calculate the BIC (Bayesian Information Criterion) for all models so far to make sure we are dropping variables in a sensible way. 

```{r}
#install.packages('flexmix')
library(flexmix)

bic_1 <- BIC(log_reg)
bic_2 <- BIC(log_reg_noSleep)
bic_3 <- BIC(log_reg_noSleep_noAlcohol)
bic_4 <- BIC(log_reg_noSleep_noAlcohol_nohighBP)
bic_5 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid)
bic_6 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften)
bic_7 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes)
bic_8 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg)
bic_9 <- BIC(log_reg_noSleep_noAlcohol_nohighBP_noPhysMid_noStressOften_Sometimes_noPreg_noJunkAlways)

isTRUE((bic_1 > bic_2)&(bic_2 > bic_3)&(bic_3 > bic_4)&(bic_4 > bic_5)&(bic_5 > bic_6)&(bic_6 > bic_7)&(bic_7 > bic_8)&(bic_8>bic_9)) #YES 
```

```{r}
length(colnames(ds_fin[,-c(2,5,6,9,18,22,23,24)])) ##At this point, we have 19 variables 
```

## 4.5. LASSO logistic regression

```{r}
y <- ds_fin$Diabetic
x <- data.matrix(ds_fin[,-12])

#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1, family="binomial")

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda #0.002622828

#produce plot of test MSE by lambda value (optional)
plot(cv_model) 

lasso_reg <- glmnet(x, y, alpha = 1, lambda = best_lambda, family = "binomial")
coef(lasso_reg)

########## LASSO drops very different coefficients than the one we would drop using stepwise selection :(( 
#An idea could be to research what simple logistic and LASSO are used for (i.e. are they used for prediction/classification
# or rather to find out which variables "weigh" more in the insurgence of, say, a sickness?) in order to figure out which one is best. 


```


## 4.6. Best Subset Selection
```{r}
library(leaps)
# response variable followed by a dot to indicate I want
# all predictor vars
regfit.full <- regsubsets(Diabetic~., data=ds_fin, nvmax=26)
summary(regfit.full)
reg.summary <- summary(regfit.full)
```

```{r}
names(reg.summary)
reg.summary$rsq
```

```{r}
# Plotting RSS, adjusted  R2,  Cp, and BIC 
# for all of the models at once will help us 
# decide which model to select. 

par(mfrow=c(2,2))

# residual sum of squares
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")

# adjusted-R^2 with its largest value
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted Rsq",type="l")
which.max(reg.summary$adjr2) # take a vec and itendifies the max value of the vector and returns the index
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)

# Mallow's Cp with its smallest value
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)

# BIC with its smallest value
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
which.min(reg.summary$bic) # take a vec and itendifies the min value of the vector and returns the index
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)

par(mfrow=c(1,1))
```

```{r}
#par(mfrow=c(2,2))
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")

plot(regfit.full,scale="bic")
coef(regfit.full,6)
#par(mfrow=c(1,1))
```

```{r}
coef(regfit.full, 10)

best.bic <- glm(Diabetic~Family_Diabetes+RegularMedicine+Pdiabetes+Age40_49+Age50_59+Age_60+PhysLow + PhysMid + StressSometimes+ BPHigh, data=ds_fin, family = binomial)
summary(best.bic)
```
The results from the best variable subset selection are in accordance with the results from the selection done by hand and verified by checking resulting correlations and up-or-down moves of the AIC. This would mean excluding the variables Smoking, Alcohol, Pregnancies, JunkAlways, StressOften, Sleep, HighBP, BPLow

## 4.7. Stepwise logistic
```{r}
model <- glm(Diabetic ~., data = train.data, family = binomial)
summary(model)
step.model <- model %>% stepAIC(trace = FALSE)
summary(step.model)
```

```{r}
best.aic <- glm(Diabetic~Family_Diabetes+Smoking+SoundSleep+RegularMedicine+Pdiabetes+Age40_49+Age50_59+Age_60+Gender_M+PhysLow + PhysHigh + StressAlways+ BPHigh, data=train.data, family = binomial)
summary(best.aic)
```
Compare the models
```{r}
# Make predictions
probabilities <- model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Prediction accuracy
observed.classes <- test.data$Diabetic
mean(predicted.classes == observed.classes)
```

```{r}
# Make predictions
probabilities <- predict(best.aic, test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1,0)
# Prediction accuracy
observed.classes <- test.data$Diabetic
mean(predicted.classes == observed.classes)
```


## 4.8. Stepwise with SMOTE
Over-sample the minority group - SMOTE
```{r}
ds_smoted <- SMOTE(ds_fin[,-12], target = ds_fin$Diabetic, K = 4, dup_size = 1.5)
ds_oversampled = ds_smoted$data
table(ds_oversampled$class)
ds_oversampled$class <- as.factor(ds_oversampled$class)
```

Performing a train-test split for model validation
```{r}
# Split the data into training and test set
set.seed(123)
training.samples_over <- ds_oversampled$class %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data_over  <- ds_oversampled[training.samples_over, ]
test.data_over <- ds_oversampled[-training.samples_over, ]
```


```{r}
model_over <- glm(class ~., data = train.data_over, family = binomial)
summary(model_over)
step.model_over <- model_over %>% stepAIC(trace = FALSE)
summary(step.model_over)
```

```{r}
best.aic <- glm(class~Family_Diabetes+Smoking+SoundSleep+RegularMedicine+Pdiabetes+Age40_49+Age50_59+Age_60+Gender_M+PhysLow + PhysHigh + StressAlways+ BPHigh, data=train.data_over, family = binomial)
summary(best.aic)
```

```{r}
# Make predictions
probabilities <- model %>% predict(test.data_over, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Prediction accuracy
observed.classes <- test.data_over$class
mean(predicted.classes == observed.classes)
```



```{r}
# Make predictions
probabilities <- predict(best.aic, test.data_over, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1,0)
# Prediction accuracy
observed.classes <- test.data_over$class
mean(predicted.classes == observed.classes)
```
The SMOTEd data set actually leads to worse prediction accuracy compared to the unbalanced data set. 
# 5. Interpretation of the data
How do we present our analysis: basically answer the questions we set in
the beginning using our results - technical details are necessary but
make them accessible - tell a story. 

Compare probability between age groups for diabetes

```{r}
aov_age <- aov(ds_fin$Diabetic~ds_fin$Age)
tukey_age <- TukeyHSD(aov_age)
plot(tukey_age)
```
